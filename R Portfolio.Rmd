---
title: "R Portfolio"
output:
  pdf_document: default
  html_document: default
---
  *Knit this R markdown file, read and understand the html file, then edit and knit the R markdown file to showcase your evidence. Save the html file for submission.*
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```

```{css, echo=FALSE}
div.warning {
  padding: 1em;
  margin: 1em 0;
  padding-left: 100px;
  background-size: 70px;
  background-repeat: no-repeat;
  background-position: 15px center;
  min-height: 120px;
  color: #000000;
    background-color: #bed3ec;
    border: solid 5px #dfedff;
}
```

```{r load-data, include=FALSE, warning=FALSE}
library(dplyr)
library(MASS)
library(tidyverse)
library(DescTools)


# Keeps 'random' consistent
set.seed(123)

# Imports data from CSVs

# Home dev
#dat1 <- read.csv('C:\\Users\\louis\\OneDrive\\Documents\\Education\\NGED\\University Work\\Year 2\\Y2-R-Portfolio\\Data\\all_sm_data.csv')
#dat2 <- read.csv(INSERT-SM-VOLUME-PATH-HERE)

# Work dev
dat1 <- read.csv('C:\\Users\\Admin\\Documents\\NGED Work\\CDP Smart Meter Data\\Data\\all_sm_data.csv')
dat2 <- read.csv('C:\\Users\\Admin\\Documents\\NGED Work\\CDP Smart Meter Data\\Data\\sm-vols-jan24-dummy.csv')

dat4 <- read.csv("Data\\historic-fault-data.csv")
```

```{r standard-functions, echo=FALSE}
# These functions will help reduce repeated code

# Removes all outliers (> 1.5x IQR) from a given column
remove_outliers <- function(df, column) {
  Q1 <- quantile(column, 0.25)
  Q3 <- quantile(column, 0.75)
  IQR <- Q3 - Q1
  
  lower_bound <- Q1 - 1.5 * IQR
  upper_bound <- Q1 + 1.5 * IQR
  
  filtered_df <- df[column >= lower_bound & column <= upper_bound,]
  
  return(filtered_df)
}
```
  

Please ensure that the evidence covers all of the following learning objectives:
  
LO1: Can apply methods concerning estimation of parameters in standard statistical models; in particular the method of moments and the maximum likelihood method.

LO2: Can perform statistical hypotheses tests using data from studies (such as t and F-tests, comparison of models and parameter values).

LO3: Can apply methods for interval estimation; in particular, exact and approximate confidence intervals based on asymptotic theory.

LO4: Can apply methods for analysing categorical data.


## Evidence 1 - Distribution of customer energy consumption

Learning objective(s) assessed: **LO1 | LO3**

```{r 1.1-visualising-the-distribution}
# For each row, calculate the power consumed per device
dat1$import_per_device <- dat1$Total_consumption_active_import / dat1$aggregated_device_count_Active

# Filter the data set to remove erroneous data values
filt <- remove_outliers(dat1, dat1$import_per_device)

# Plot histogram of the values observed in power consumed per device
hist(filt$import_per_device, 
     breaks = seq(min(filt$import_per_device), max(filt$import_per_device), length.out = 30),
     main = "Distribution of average energy consumption per device per half hour",
     xlab = "Mean energy consumption (Wh)")
```

By dividing the total energy consumption by the number of active devices for each row we can find the the average (mean) energy consumption for each device for each half-hour interval. After removing outliers (greater than 1.5x the interquartile range) and plotting a histogram for the distribution of these values we can see a fairly normal graph.

To find good estimates for the mean and standard deviation of this distribution we can use the maximum likelihood method:

```{r 1.2-mle}
# Fit normal distribution to the filtered data (find the maximum likelihood estimators for mean and standard deviation)
fit_norm <- fitdistr(filt$import_per_device, densfun = "normal")
print(fit_norm)
```

The maximum likelihood estimators for the mean ($\hat{\mu}$) and the standard deviation ($\hat{\sigma}$) are shown above with the standard error shown below them. A 95% confidence interval can be calculated for each estimator to help quantify the uncertainty of the estimations for the population parameters derived from the sample:

```{r 1.3-confidence-intervals}
# Finds the 95% confidence intervals for the MLEs of both the mean and standard deviation
conf_intervals <- confint(fit_norm, level=0.95)
print(conf_intervals)
```

By inserting these estimates into a random normal distribution generator we can compare the estimated shape of the population with the shape we observe in the sample.

```{r 1.4-compare-with-random-normal}
# Plot a random normal distribution using the MLEs to compare against actual distribution
test_data <- rnorm(10000,mean=fit_norm$estimate['mean'],sd=fit_norm$estimate['sd'])
hist(test_data, breaks = 30,
     main = "Distribution of random normal (μ = 223.6, σ = 92.2)",
     xlab = "Generated X value")
```

  
## Evidence 2 - Smart meter uptake in different regions
  
Learning objective(s) assessed: **LO 2 | LO4**

```{r 2.1-contingency-table}
# Grabs the names of all 4 different licence areas
LAs = unique(dat2$Licence_Area)

# Loops through each row in the data set and finds the number of smart and non-smart meters for each region
tab_dat = c()
for (LA in LAs) {
  filtered <- dat2[dat2$Licence_Area == LA,]
  
  S1 = sum(filtered$SMETS1)
  S2 = sum(filtered$SMETS2)
  nonS = sum(filtered$NON_SMART)
  
  row_dat = c(S1, S2, nonS)
  
  tab_dat = append(tab_dat, row_dat)
}

# This data is then stored in a 3x4 matrix (contingency table)
tab <- matrix(tab_dat, nrow=3, ncol=length(LAs), dimnames=list(c("SMETS1","SMETS2","NON SMART"),LAs))

print(tab)
```

Using this table we can test for the independence of the variables 'Licence Area' and 'Meter Type'.

$H_0:$ The meter type is independent of which licence area it resides in.
$vs.$
$H_1:$ The meter type depends on which licence area the meter resides in.

Under $H_0$ for large $n$ (which is applicable here as n = 8,124,921), 

$-2\log(\Lambda)\sim \chi_{(J-1)(K-1)}^{2} \space approximately,$

$-2\log(\Lambda)=2\sum\limits_{j=1}^{J}\sum\limits_{k=1}^{K}y_{jk}\log\left(\frac{y_{jk}}{\hat{e}_{jk}}\right)$

$\hat{e}_{jk}=\frac{y_{j\bullet}y_{\bullet k}}{n}$

```{r 2.2-G-Test}
# Uses a G-Test (with no correction) to test the independence of the licence area and the type of meter used by customers
results <- GTest(tab,correct="none")

print(results)
```
The p-value for this test is <2.2e-16 which is much less than 0.01. This suggests there is overwhelming evidence to reject the null hypothesis ($H_0$) and we can say the variables are not independent. This suggests that different areas have different levels of uptake of smart meters compared to others. 

## Evidence 3
  
Learning objective(s) assessed: **???**
  
Please add new evidence and subsections as necessary.

```{r}
# Any R code here
```

**Any writing here**



## Evidence 4 - Time until next fault
  
Learning objective(s) assessed: **LO1**

Another important area of the business is fault prediction. 

```{r 4.1-aggregation}
# Aggregate by fault
faults <- dat4 %>%
  group_by(incident_ref) %>%
  summarise(licence_area = first(licence_area), # Should all be the same anyway
            interruption_date = min(interruption_date), # Gets earliest interruption date (will be from stage 1)
            restoration_date = max(restoration_date), # Gets latest restoration date (will be from last stage)
            voltage = first(voltage), # Should all be the same anyway
            report_ci = max(report_ci)) # Gets the largest value (the max number of customers off simultaneously)
```

By using historic fault records, we can calculate the time intervals (in seconds) between faults occurring on our network. Here we could split the data into seperate parts for each licence area or GSP/BSP but to keep things simple we will use the whole network.

```{r 4.2-intervals}
# Convert datetime column to POSIXct
faults$interruption_date <- as.POSIXct(faults$interruption_date, format = "%d/%m/%Y %H:%M:%S")

# Order data by datetime column
faults <- faults[order(faults$interruption_date), ]

faults$fault_interval <- NA  # Initialize the new column with NA

for (i in 2:nrow(faults)) {
  # Calculate time difference in minutes between current row and previous row
  interval <- difftime(faults$interruption_date[i], faults$interruption_date[i - 1], units = "secs")
  
  # Assign the calculated interval to the corresponding row in the new column
  faults$fault_interval[i] <- as.numeric(interval)
}
```

```{r 4.3-histogram}
# Remove the initial row (where interval = NA)
faults <- na.omit(faults)

# Remove any outliers from the fault interval column
faults <- remove_outliers(faults, faults$fault_interval)

# Plot distribution of time until
hist(faults$fault_interval, breaks=20, main="Distribution of time until next fault", xlab="Time until next fault (secs)")
```

Plotting the distribution of intervals on a histogram we can see a clear exponential decay. With this knowledge, we can now try to fit our data to an exponential distribution to find an estimate for the rate ($\lambda$) parameter.

```{r 4.4-mle-exp}
fit_exp <- fitdistr(faults$fault_interval, densfun="exponential")

print(fit_exp)
```

Once we have our estimate we can check it against a randomly generated exponential distribution with the same rate.

```{r 4.5-compare-rand-exp}
test_dat <- rexp(10000, rate=fit_exp$estimate)

hist(test_dat, xlim = c(0, 1000), breaks = 50, main = "Distribution of random Exponential (lambda = 0.002978)", xlab = "Generated X value")
```

The shape of the distribution is very similar which indicates the value found is a good estimation for $\lambda$. An alternative method for parameter estimation is the 'Method of Moments'. This method simply equates theoretical and sample moments - choosing to ensure the distribution ‘matches’ the data in the sense of equating the moments. For a single parameter this means solving:
$E(X) = \bar{X}$

The expected value of X where X ~ $Exp(\lambda)$ is defined as:
$E(X) = \frac{1}{\lambda}$

We can find $\bar{X}$ (sample mean) by summing the time intervals and then dividing the result by the number of intervals in our data set.

```{r 4.6-mom-exp}
sum <- sum(faults$fault_interval)
count <- length(faults$fault_interval)

sample_mean <- mean(faults$fault_interval)

lambda_estimate <- 1/sample_mean
```

$\bar{X}$ = `r sum` / `r count` = `r sample_mean`

Using the equations above we can say

`r sample mean` = $\frac{1}{\lambda}$

$\therefore \space \hat\lambda$ = 1 / `r sample_mean` = `r lambda_estimate`

This estimate is coincidentally the same as the estimate derived using the maximum likelihood estimator methods above.
